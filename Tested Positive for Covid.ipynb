{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mikes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mikes/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/mikes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mikes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "wl = WordNetLemmatizer()\n",
    "snow = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vd = pd.read_csv(\"/home/mikes/Windows/Documents/tmp/VAERS data deaths only Edited for NLP for HAD COVID.csv\", \n",
    "#                          delimiter = \",\", \n",
    "#                          encoding = \"ISO-8859-1\")\n",
    "\n",
    "vd = pd.read_csv(\"./VAERS data deaths only Edited for NLP for HAD COVID.csv\", \n",
    "                         delimiter = \",\", \n",
    "                         encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VAERS_ID', 'RECVDATE', 'STATE', 'AGE_YRS', 'CAGE_YR', 'CAGE_MO', 'SEX',\n",
       "       'RPT_DATE', 'SYMPTOM_TEXT', 'SYMPTOM_TEXT_TARGET', 'DIED', 'DATEDIED',\n",
       "       'L_THREAT', 'ER_VISIT', 'HOSPITAL', 'HOSPDAYS', 'X_STAY', 'DISABLE',\n",
       "       'RECOVD', 'VAX_DATE', 'ONSET_DATE', 'NUMDAYS', 'LAB_DATA',\n",
       "       'LAB_DATA_TARGET', 'V_ADMINBY', 'V_FUNDBY', 'OTHER_MEDS',\n",
       "       'OTHER_MEDS_TARGET', 'CUR_ILL', 'CUR_ILL_TARGET', 'HISTORY',\n",
       "       'PRIOR_VAX', 'SPLTTYPE', 'FORM_VERS', 'TODAYS_DATE', 'BIRTH_DEFECT',\n",
       "       'OFC_VISIT', 'ER_ED_VISIT', 'ALLERGIES'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vd. columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase and remove punctuations and characters and then strip\n",
    "def preprocess(text):\n",
    "    text = text.lower() #lowercase text\n",
    "    text=text.strip()  #get rid of leading/trailing whitespace \n",
    "    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space. Careful since punctuation can sometime be useful\n",
    "    text = re.sub('\\s+', ' ', text)  #Remove extra space and tabs\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) #[0-9] matches any digit (0 to 10000...)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) #matches any digit from 0 to 100000..., \\D matches non-digits\n",
    "    text = re.sub(r'\\s+',' ',text) #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace \n",
    "    \n",
    "    return text\n",
    "\n",
    "#1. STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "#2. STEMMING \n",
    "# Initialize the stemmer\n",
    "#snow = SnowballStemmer('english')\n",
    "def stemming(string):\n",
    "    a=[snow.stem(i) for i in word_tokenize(string) ]\n",
    "    return \" \".join(a)\n",
    "\n",
    "#3. LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "#wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "def finalpreprocess(string):\n",
    "    string = str(string)\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "#    return lemmatizer(preprocess(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"apple\" in stopwords.words('english'): print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.The F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.\n",
    "\n",
    "The support is the number of occurrences of each class in y_true.\n",
    "\n",
    "If pos_label is None and in binary classification, this function returns the average precision, recall and F-measure if average is one of 'micro', 'macro', 'weighted' or 'samples'.\n",
    "\n",
    "Read more in the [User Guide](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def logistic_regression_tfidf(df_train, df_test, ID, TEXT, TARGET, perc_cutoff = 80):\n",
    "    if perc_cutoff > 1: perc_cutoff = perc_cutoff/100\n",
    "    df_train = df_train.loc[df_train[TARGET].notnull(), [ID, TEXT, TARGET ]]\n",
    "    df_train['word_count'] = df_train[TEXT].apply(lambda x: len(str(x).split()))\n",
    "    #print(df_train[df_train[TARGET]==1]['word_count'].mean()) #Positive match\n",
    "    #print(df_train[df_train[TARGET]==0]['word_count'].mean()) #Not positive match \n",
    "    df_train['clean_text'] = df_train[TEXT].apply(lambda x: finalpreprocess(x))\n",
    "    # create Word2vec model\n",
    "    #  here words_f should be a list containing words from each document. say 1st row of the list is words from the 1st document/sentence\n",
    "    #  length of words_f is number of documents/sentences in your dataset\n",
    "    df_train['clean_text_tok']=[nltk.word_tokenize(i) for i in df_train['clean_text']] #convert preprocessed sentence to tokenized sentence\n",
    "    model = Word2Vec(df_train['clean_text_tok'],min_count=1)  #min_count=1 means word should be present at least across all documents,\n",
    "    #if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
    "    w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  #combination of word and its vector\n",
    "\n",
    "    #SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    "    # Input: \"reviewText\", \"rating\" and \"time\"\n",
    "    # Target: \"log_votes\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df_train[\"clean_text\"],\n",
    "                                                      df_train[TARGET],\n",
    "                                                      test_size=0.2,\n",
    "                                                      shuffle=True)\n",
    "    X_train_tok= [nltk.word_tokenize(i) for i in X_train]  #for word2vec\n",
    "    X_val_tok= [nltk.word_tokenize(i) for i in X_val]      #for word2vec\n",
    "    #TF-IDF\n",
    "    # Convert x_train to vector since model can only run on numbers and not words- Fit and transform\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) #tfidf runs on non-tokenized sentences unlike word2vec\n",
    "    # Only transform x_test (not fit and transform)\n",
    "    X_val_vectors_tfidf = tfidf_vectorizer.transform(X_val) #Don't fit() your TfidfVectorizer to your test data: it will \n",
    "    #change the word-indexes & weights to match test data. Rather, fit on the training data, then use the same train-data-\n",
    "    #fit model on the test data, to reflect the fact you're analyzing the test data only based on what was learned without \n",
    "    #it, and the have compatible\n",
    "\n",
    "    ######################################################################\n",
    "    \n",
    "    #FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "    lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "    lr_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "    #Predict y value for test dataset\n",
    "    y_predict = lr_tfidf.predict(X_val_vectors_tfidf)\n",
    "    y_prob = lr_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
    "\n",
    "\n",
    "    print(classification_report(y_val,y_predict))\n",
    "    print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print('AUC:', roc_auc)\n",
    "\n",
    "    ### EXAMINE TEST SET WITH MODEL. This includes the data from training (both pos and neg targets) ###\n",
    "    df_test['clean_text'] = vd['SYMPTOM_TEXT'].apply(lambda x: finalpreprocess(x)) #preprocess the data\n",
    "    X_test=df_test['clean_text'] \n",
    "    X_vector=tfidf_vectorizer.transform(X_test) #converting X_test to vector\n",
    "    y_predict = lr_tfidf.predict(X_vector)      #use the trained model on X_vector\n",
    "    y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "    df_test['predict_prob']= y_prob\n",
    "    df_test['target']= y_predict\n",
    "    # display(df_test[['VAERS_ID','SYMPTOM_TEXT','predict_prob','target']].head())\n",
    "    # #final=df_test[['id','target']].reset_index(drop=True)\n",
    "    # #final.to_csv('submission.csv')\n",
    "    df_results = df_test[(\n",
    "                            (df_test[\"target\"] == 1) & \n",
    "                            (df_test[\"predict_prob\"] >= perc_cutoff)\n",
    "    )]\n",
    "    num_found = df_results.shape[0]\n",
    "    num_total = df_test.shape[0]\n",
    "    print(\"{}/{} ({})\".format(num_found, num_total, num_found/num_total))\n",
    "    return num_found,df_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(df, ID, TEXT, TARGET, iterations = 10):\n",
    "    results = []\n",
    "    for i in range(0,iterations,1):\n",
    "        print(); print(\" Iteration {} {} ==========\".format(i, TEXT))\n",
    "        df_train = df.loc[vd[TARGET].notnull(), [ID,TEXT,TARGET]]\n",
    "        df_train['word_count'] = df_train[TEXT].apply(lambda x: len(str(x).split()))\n",
    "        print(\"Average word count, positive targets: {}\".format(df_train[df_train[TARGET]==1]['word_count'].mean())) #Has covid\n",
    "        print(\"Average word count, negative targets: {}\".format(df_train[df_train[TARGET]==0]['word_count'].mean())) #Has covid\n",
    "        num_found,df_results = logistic_regression_tfidf(df_train, df, ID,TEXT,TARGET)\n",
    "        results.append((df_results, num_found, num_found/df.shape[0]))\n",
    "\n",
    "    for i in results:\n",
    "        print(i[1], i[2])\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iteration 0 LAB_DATA ==========\n",
      "Average word count, positive targets: 19.34285714285714\n",
      "Average word count, negative targets: 51.73076923076923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.83      0.87        24\n",
      "         1.0       0.80      0.89      0.84        18\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.85      0.86      0.86        42\n",
      "weighted avg       0.86      0.86      0.86        42\n",
      "\n",
      "Confusion Matrix: [[20  4]\n",
      " [ 2 16]]\n",
      "AUC: 0.9305555555555556\n",
      "70/5184 (0.013503086419753086)\n",
      "\n",
      " Iteration 1 LAB_DATA ==========\n",
      "Average word count, positive targets: 19.34285714285714\n",
      "Average word count, negative targets: 51.73076923076923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.82      0.80        22\n",
      "         1.0       0.79      0.75      0.77        20\n",
      "\n",
      "    accuracy                           0.79        42\n",
      "   macro avg       0.79      0.78      0.78        42\n",
      "weighted avg       0.79      0.79      0.79        42\n",
      "\n",
      "Confusion Matrix: [[18  4]\n",
      " [ 5 15]]\n",
      "AUC: 0.9363636363636363\n",
      "61/5184 (0.011766975308641976)\n",
      "\n",
      " Iteration 2 LAB_DATA ==========\n",
      "Average word count, positive targets: 19.34285714285714\n",
      "Average word count, negative targets: 51.73076923076923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.89      0.85        19\n",
      "         1.0       0.90      0.83      0.86        23\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.86      0.86      0.86        42\n",
      "weighted avg       0.86      0.86      0.86        42\n",
      "\n",
      "Confusion Matrix: [[17  2]\n",
      " [ 4 19]]\n",
      "AUC: 0.9450800915331808\n",
      "50/5184 (0.009645061728395061)\n",
      "\n",
      " Iteration 3 LAB_DATA ==========\n",
      "Average word count, positive targets: 19.34285714285714\n",
      "Average word count, negative targets: 51.73076923076923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.89      0.76        19\n",
      "         1.0       0.88      0.61      0.72        23\n",
      "\n",
      "    accuracy                           0.74        42\n",
      "   macro avg       0.76      0.75      0.74        42\n",
      "weighted avg       0.77      0.74      0.73        42\n",
      "\n",
      "Confusion Matrix: [[17  2]\n",
      " [ 9 14]]\n",
      "AUC: 0.88558352402746\n",
      "70/5184 (0.013503086419753086)\n",
      "\n",
      " Iteration 4 LAB_DATA ==========\n",
      "Average word count, positive targets: 19.34285714285714\n",
      "Average word count, negative targets: 51.73076923076923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.93      0.82        15\n",
      "         1.0       0.96      0.81      0.88        27\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.85      0.87      0.85        42\n",
      "weighted avg       0.88      0.86      0.86        42\n",
      "\n",
      "Confusion Matrix: [[14  1]\n",
      " [ 5 22]]\n",
      "AUC: 0.9555555555555557\n",
      "43/5184 (0.008294753086419753)\n",
      "\n",
      " Iteration 5 LAB_DATA ==========\n",
      "Average word count, positive targets: 19.34285714285714\n",
      "Average word count, negative targets: 51.73076923076923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.95      0.90        20\n",
      "         1.0       0.95      0.86      0.90        22\n",
      "\n",
      "    accuracy                           0.90        42\n",
      "   macro avg       0.91      0.91      0.90        42\n",
      "weighted avg       0.91      0.90      0.90        42\n",
      "\n",
      "Confusion Matrix: [[19  1]\n",
      " [ 3 19]]\n",
      "AUC: 0.9522727272727273\n",
      "94/5184 (0.018132716049382717)\n",
      "\n",
      " Iteration 6 LAB_DATA ==========\n",
      "Average word count, positive targets: 19.34285714285714\n",
      "Average word count, negative targets: 51.73076923076923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.96      0.92        24\n",
      "         1.0       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           0.90        42\n",
      "   macro avg       0.91      0.90      0.90        42\n",
      "weighted avg       0.91      0.90      0.90        42\n",
      "\n",
      "Confusion Matrix: [[23  1]\n",
      " [ 3 15]]\n",
      "AUC: 0.9537037037037037\n",
      "87/5184 (0.01678240740740741)\n",
      "\n",
      " Iteration 7 LAB_DATA ==========\n",
      "Average word count, positive targets: 19.34285714285714\n",
      "Average word count, negative targets: 51.73076923076923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.93      0.72        15\n",
      "         1.0       0.94      0.63      0.76        27\n",
      "\n",
      "    accuracy                           0.74        42\n",
      "   macro avg       0.76      0.78      0.74        42\n",
      "weighted avg       0.82      0.74      0.74        42\n",
      "\n",
      "Confusion Matrix: [[14  1]\n",
      " [10 17]]\n",
      "AUC: 0.8864197530864197\n",
      "67/5184 (0.012924382716049383)\n",
      "\n",
      " Iteration 8 LAB_DATA ==========\n",
      "Average word count, positive targets: 19.34285714285714\n",
      "Average word count, negative targets: 51.73076923076923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.94      0.77        16\n",
      "         1.0       0.95      0.69      0.80        26\n",
      "\n",
      "    accuracy                           0.79        42\n",
      "   macro avg       0.80      0.81      0.78        42\n",
      "weighted avg       0.83      0.79      0.79        42\n",
      "\n",
      "Confusion Matrix: [[15  1]\n",
      " [ 8 18]]\n",
      "AUC: 0.9326923076923077\n",
      "45/5184 (0.008680555555555556)\n",
      "\n",
      " Iteration 9 LAB_DATA ==========\n",
      "Average word count, positive targets: 19.34285714285714\n",
      "Average word count, negative targets: 51.73076923076923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.71      0.79        24\n",
      "         1.0       0.70      0.89      0.78        18\n",
      "\n",
      "    accuracy                           0.79        42\n",
      "   macro avg       0.80      0.80      0.79        42\n",
      "weighted avg       0.81      0.79      0.79        42\n",
      "\n",
      "Confusion Matrix: [[17  7]\n",
      " [ 2 16]]\n",
      "AUC: 0.9305555555555556\n",
      "70/5184 (0.013503086419753086)\n",
      "70 0.013503086419753086\n",
      "61 0.011766975308641976\n",
      "50 0.009645061728395061\n",
      "70 0.013503086419753086\n",
      "43 0.008294753086419753\n",
      "94 0.018132716049382717\n",
      "87 0.01678240740740741\n",
      "67 0.012924382716049383\n",
      "45 0.008680555555555556\n",
      "70 0.013503086419753086\n",
      "\n",
      " Iteration 0 SYMPTOM_TEXT ==========\n",
      "Average word count, positive targets: 100.20792079207921\n",
      "Average word count, negative targets: 104.03960396039604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.96      0.94        23\n",
      "         1.0       0.94      0.89      0.91        18\n",
      "\n",
      "    accuracy                           0.93        41\n",
      "   macro avg       0.93      0.92      0.93        41\n",
      "weighted avg       0.93      0.93      0.93        41\n",
      "\n",
      "Confusion Matrix: [[22  1]\n",
      " [ 2 16]]\n",
      "AUC: 0.9541062801932367\n",
      "231/5184 (0.04456018518518518)\n",
      "\n",
      " Iteration 1 SYMPTOM_TEXT ==========\n",
      "Average word count, positive targets: 100.20792079207921\n",
      "Average word count, negative targets: 104.03960396039604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.88      0.89        24\n",
      "         1.0       0.83      0.88      0.86        17\n",
      "\n",
      "    accuracy                           0.88        41\n",
      "   macro avg       0.87      0.88      0.88        41\n",
      "weighted avg       0.88      0.88      0.88        41\n",
      "\n",
      "Confusion Matrix: [[21  3]\n",
      " [ 2 15]]\n",
      "AUC: 0.9607843137254902\n",
      "230/5184 (0.04436728395061729)\n",
      "\n",
      " Iteration 2 SYMPTOM_TEXT ==========\n",
      "Average word count, positive targets: 100.20792079207921\n",
      "Average word count, negative targets: 104.03960396039604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.90      0.91        29\n",
      "         1.0       0.77      0.83      0.80        12\n",
      "\n",
      "    accuracy                           0.88        41\n",
      "   macro avg       0.85      0.86      0.86        41\n",
      "weighted avg       0.88      0.88      0.88        41\n",
      "\n",
      "Confusion Matrix: [[26  3]\n",
      " [ 2 10]]\n",
      "AUC: 0.9741379310344828\n",
      "231/5184 (0.04456018518518518)\n",
      "\n",
      " Iteration 3 SYMPTOM_TEXT ==========\n",
      "Average word count, positive targets: 100.20792079207921\n",
      "Average word count, negative targets: 104.03960396039604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.96      0.92        23\n",
      "         1.0       0.94      0.83      0.88        18\n",
      "\n",
      "    accuracy                           0.90        41\n",
      "   macro avg       0.91      0.89      0.90        41\n",
      "weighted avg       0.91      0.90      0.90        41\n",
      "\n",
      "Confusion Matrix: [[22  1]\n",
      " [ 3 15]]\n",
      "AUC: 0.9685990338164251\n",
      "269/5184 (0.051890432098765434)\n",
      "\n",
      " Iteration 4 SYMPTOM_TEXT ==========\n",
      "Average word count, positive targets: 100.20792079207921\n",
      "Average word count, negative targets: 104.03960396039604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.81      0.85        21\n",
      "         1.0       0.82      0.90      0.86        20\n",
      "\n",
      "    accuracy                           0.85        41\n",
      "   macro avg       0.86      0.85      0.85        41\n",
      "weighted avg       0.86      0.85      0.85        41\n",
      "\n",
      "Confusion Matrix: [[17  4]\n",
      " [ 2 18]]\n",
      "AUC: 0.9666666666666667\n",
      "273/5184 (0.052662037037037035)\n",
      "\n",
      " Iteration 5 SYMPTOM_TEXT ==========\n",
      "Average word count, positive targets: 100.20792079207921\n",
      "Average word count, negative targets: 104.03960396039604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.95      0.95        19\n",
      "         1.0       0.95      0.95      0.95        22\n",
      "\n",
      "    accuracy                           0.95        41\n",
      "   macro avg       0.95      0.95      0.95        41\n",
      "weighted avg       0.95      0.95      0.95        41\n",
      "\n",
      "Confusion Matrix: [[18  1]\n",
      " [ 1 21]]\n",
      "AUC: 0.9688995215311005\n",
      "252/5184 (0.04861111111111111)\n",
      "\n",
      " Iteration 6 SYMPTOM_TEXT ==========\n",
      "Average word count, positive targets: 100.20792079207921\n",
      "Average word count, negative targets: 104.03960396039604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.94      0.85        18\n",
      "         1.0       0.95      0.78      0.86        23\n",
      "\n",
      "    accuracy                           0.85        41\n",
      "   macro avg       0.86      0.86      0.85        41\n",
      "weighted avg       0.87      0.85      0.85        41\n",
      "\n",
      "Confusion Matrix: [[17  1]\n",
      " [ 5 18]]\n",
      "AUC: 0.9541062801932367\n",
      "215/5184 (0.04147376543209876)\n",
      "\n",
      " Iteration 7 SYMPTOM_TEXT ==========\n",
      "Average word count, positive targets: 100.20792079207921\n",
      "Average word count, negative targets: 104.03960396039604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.94      0.89        18\n",
      "         1.0       0.95      0.87      0.91        23\n",
      "\n",
      "    accuracy                           0.90        41\n",
      "   macro avg       0.90      0.91      0.90        41\n",
      "weighted avg       0.91      0.90      0.90        41\n",
      "\n",
      "Confusion Matrix: [[17  1]\n",
      " [ 3 20]]\n",
      "AUC: 0.9685990338164251\n",
      "268/5184 (0.05169753086419753)\n",
      "\n",
      " Iteration 8 SYMPTOM_TEXT ==========\n",
      "Average word count, positive targets: 100.20792079207921\n",
      "Average word count, negative targets: 104.03960396039604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.94      0.87        18\n",
      "         1.0       0.95      0.83      0.88        23\n",
      "\n",
      "    accuracy                           0.88        41\n",
      "   macro avg       0.88      0.89      0.88        41\n",
      "weighted avg       0.89      0.88      0.88        41\n",
      "\n",
      "Confusion Matrix: [[17  1]\n",
      " [ 4 19]]\n",
      "AUC: 0.9685990338164252\n",
      "202/5184 (0.03896604938271605)\n",
      "\n",
      " Iteration 9 SYMPTOM_TEXT ==========\n",
      "Average word count, positive targets: 100.20792079207921\n",
      "Average word count, negative targets: 104.03960396039604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.91      0.91        23\n",
      "         1.0       0.89      0.89      0.89        18\n",
      "\n",
      "    accuracy                           0.90        41\n",
      "   macro avg       0.90      0.90      0.90        41\n",
      "weighted avg       0.90      0.90      0.90        41\n",
      "\n",
      "Confusion Matrix: [[21  2]\n",
      " [ 2 16]]\n",
      "AUC: 0.9516908212560387\n",
      "246/5184 (0.047453703703703706)\n",
      "231 0.04456018518518518\n",
      "230 0.04436728395061729\n",
      "231 0.04456018518518518\n",
      "269 0.051890432098765434\n",
      "273 0.052662037037037035\n",
      "252 0.04861111111111111\n",
      "215 0.04147376543209876\n",
      "268 0.05169753086419753\n",
      "202 0.03896604938271605\n",
      "246 0.047453703703703706\n"
     ]
    }
   ],
   "source": [
    "LAB_DATA_RESULTS = run_analysis(vd, 'VAERS_ID','LAB_DATA','LAB_DATA_TARGET')\n",
    "\n",
    "LAB_SYMPTOM_TEXT_RESULTS = run_analysis(vd, 'VAERS_ID','SYMPTOM_TEXT','SYMPTOM_TEXT_TARGET')\n",
    "\n",
    "\n",
    "# results = []\n",
    "# for i in range(0,10,1):\n",
    "#     df_train = vd.loc[vd[TARGET].notnull(), [ID,TEXT,TARGET]]\n",
    "#     df_train['word_count'] = df_train[TEXT].apply(lambda x: len(str(x).split()))\n",
    "#     print(\"Average word count, positive targets: {}\".format(df_train[df_train[TARGET]==1]['word_count'].mean())) #Has covid\n",
    "#     print(\"Average word count, negative targets: {}\".format(df_train[df_train[TARGET]==0]['word_count'].mean())) #Has covid\n",
    "#     num_found,df_results_LAB_DATA = logistic_regression_tfidf(df_train, vd, ID,TEXT,TARGET)\n",
    "#     results.append(num_found/5184)\n",
    "\n",
    "# LAB_DATA_RESULTS = results\n",
    "# for i in LAB_DATA_RESULTS:\n",
    "#     print(\"{}%\".format(i*100))\n",
    "    \n",
    "# print(\"Average: {}%\".format( (sum(results)/len(results)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0)==========================\n",
      "df_results_SYMPTOM_TEXT= (231, 42)\n",
      "df_results_LAB_DATA= (70, 42)\n",
      "SYMPTOM_TEXT + LAB_DATA rows =  301\n",
      "SYMPTOM_TEXT + LAB_DATA, duplicates dropped = (244, 39)\n",
      "Unique VAERS IDs in SYMPTOM_TEXT + LAB_DATA =  231.79999999999998\n",
      "\n",
      "(1)==========================\n",
      "df_results_SYMPTOM_TEXT= (230, 42)\n",
      "df_results_LAB_DATA= (61, 42)\n",
      "SYMPTOM_TEXT + LAB_DATA rows =  291\n",
      "SYMPTOM_TEXT + LAB_DATA, duplicates dropped = (237, 39)\n",
      "Unique VAERS IDs in SYMPTOM_TEXT + LAB_DATA =  225.14999999999998\n",
      "\n",
      "(2)==========================\n",
      "df_results_SYMPTOM_TEXT= (231, 42)\n",
      "df_results_LAB_DATA= (50, 42)\n",
      "SYMPTOM_TEXT + LAB_DATA rows =  281\n",
      "SYMPTOM_TEXT + LAB_DATA, duplicates dropped = (237, 39)\n",
      "Unique VAERS IDs in SYMPTOM_TEXT + LAB_DATA =  225.14999999999998\n",
      "\n",
      "(3)==========================\n",
      "df_results_SYMPTOM_TEXT= (269, 42)\n",
      "df_results_LAB_DATA= (70, 42)\n",
      "SYMPTOM_TEXT + LAB_DATA rows =  339\n",
      "SYMPTOM_TEXT + LAB_DATA, duplicates dropped = (277, 39)\n",
      "Unique VAERS IDs in SYMPTOM_TEXT + LAB_DATA =  263.15\n",
      "\n",
      "(4)==========================\n",
      "df_results_SYMPTOM_TEXT= (273, 42)\n",
      "df_results_LAB_DATA= (43, 42)\n",
      "SYMPTOM_TEXT + LAB_DATA rows =  316\n",
      "SYMPTOM_TEXT + LAB_DATA, duplicates dropped = (277, 39)\n",
      "Unique VAERS IDs in SYMPTOM_TEXT + LAB_DATA =  263.15\n",
      "\n",
      "(5)==========================\n",
      "df_results_SYMPTOM_TEXT= (252, 42)\n",
      "df_results_LAB_DATA= (94, 42)\n",
      "SYMPTOM_TEXT + LAB_DATA rows =  346\n",
      "SYMPTOM_TEXT + LAB_DATA, duplicates dropped = (279, 39)\n",
      "Unique VAERS IDs in SYMPTOM_TEXT + LAB_DATA =  265.05\n",
      "\n",
      "(6)==========================\n",
      "df_results_SYMPTOM_TEXT= (215, 42)\n",
      "df_results_LAB_DATA= (87, 42)\n",
      "SYMPTOM_TEXT + LAB_DATA rows =  302\n",
      "SYMPTOM_TEXT + LAB_DATA, duplicates dropped = (233, 39)\n",
      "Unique VAERS IDs in SYMPTOM_TEXT + LAB_DATA =  221.35\n",
      "\n",
      "(7)==========================\n",
      "df_results_SYMPTOM_TEXT= (268, 42)\n",
      "df_results_LAB_DATA= (67, 42)\n",
      "SYMPTOM_TEXT + LAB_DATA rows =  335\n",
      "SYMPTOM_TEXT + LAB_DATA, duplicates dropped = (279, 39)\n",
      "Unique VAERS IDs in SYMPTOM_TEXT + LAB_DATA =  265.05\n",
      "\n",
      "(8)==========================\n",
      "df_results_SYMPTOM_TEXT= (202, 42)\n",
      "df_results_LAB_DATA= (45, 42)\n",
      "SYMPTOM_TEXT + LAB_DATA rows =  247\n",
      "SYMPTOM_TEXT + LAB_DATA, duplicates dropped = (208, 39)\n",
      "Unique VAERS IDs in SYMPTOM_TEXT + LAB_DATA =  197.6\n",
      "\n",
      "(9)==========================\n",
      "df_results_SYMPTOM_TEXT= (246, 42)\n",
      "df_results_LAB_DATA= (70, 42)\n",
      "SYMPTOM_TEXT + LAB_DATA rows =  316\n",
      "SYMPTOM_TEXT + LAB_DATA, duplicates dropped = (256, 39)\n",
      "Unique VAERS IDs in SYMPTOM_TEXT + LAB_DATA =  243.2\n",
      "\n",
      "Average number of covid positives in VAERS data:  240.06499999999997\n",
      "Average percent of covid positives in VAERS data:  4.63088348765432\n"
     ]
    }
   ],
   "source": [
    "LAB_SYMPTOM_TEXT_RESULTS = LAB_SYMPTOM_TEXT # Named it wrong originally :D Remove for next run :D\n",
    "\n",
    "results = {\"num_covid_infections\":[], \"ratio_to_vd\":[]}\n",
    "for i in range(0,len(LAB_SYMPTOM_TEXT_RESULTS), 1):     \n",
    "    print(); print(\"({})==========================\".format(i))\n",
    "    df1 = LAB_SYMPTOM_TEXT_RESULTS[i][0]\n",
    "    df1 = df1[df1[\"target\"] == 1]\n",
    "    print(\"df_results_SYMPTOM_TEXT=\",df1.shape)\n",
    "    \n",
    "    df2 = LAB_DATA_RESULTS[i][0]\n",
    "    df2 = df2[df2[\"target\"] == 1]\n",
    "    print(\"df_results_LAB_DATA=\",df2.shape)\n",
    "\n",
    "\n",
    "    print(\"SYMPTOM_TEXT + LAB_DATA rows = \", df1.shape[0] + df2.shape[0])\n",
    "    for key in [\"clean_text\",\"predict_prob\",\"target\"]:\n",
    "        df1 = df1.drop([key], 1)\n",
    "        df2 = df2.drop([key], 1)\n",
    "\n",
    "    df = pd.concat([df1,df2]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    print(\"SYMPTOM_TEXT + LAB_DATA, duplicates dropped =\", df.shape)\n",
    "    # Assuming 5% error, so removing 5%\n",
    "    num_covid_infections = len(df[\"VAERS_ID\"].unique()) * 0.95\n",
    "    print(\"Unique VAERS IDs in SYMPTOM_TEXT + LAB_DATA = \",num_covid_infections)\n",
    "    \n",
    "    results[\"num_covid_infections\"].append(num_covid_infections)\n",
    "    results[\"ratio_to_vd\"].append((num_covid_infections/vd.shape[0]))\n",
    "\n",
    "print()    \n",
    "\n",
    "print(\"Average number of covid positives in VAERS data (deaths only): \",  np.mean(results[\"num_covid_infections\"]))\n",
    "print(\"Average percent of covid positives in VAERS data (deaths only): \", np.mean(results[\"ratio_to_vd\"])*100)\n",
    "                   \n",
    "                   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods:\n",
    "The VAERS data was downloaded for all of 2021, first week of July from https://vaers.hhs.gov/data.html. Data was filtered to include only rows where column \"DIED\" == \"Y\".\n",
    "\n",
    "Two columns were pertinent to the disucsion: \"SYMPTOM_TEXT\" and \"LAB_DATA\". Information verifying that patient had covid may appear in one or both columns, so each column was processed separately. The results of each search were then concatenated together, and duspliactes were removed based on \"VAERS_ID\". The row counts were then tallied.  \n",
    "\n",
    "The model was trained by manually adding the columns: \"SYMPTOM_TEXT_TARGET\" and \"LAB_DATA_TARGET\". These columns were manually marked with a 0 (not infected with covid) or 1 (infected with covid) based on the contents of the \"SYMPTOM_TEXT\" and \"LAB_DATA\" columns (respectively.) A pateint was considered \"Infected with covid\" if they had a positive test within 30 days prior to the vaccination, after shot, or if the text annotated that the patient ad covid symptoms or died of a covid related symptom.\n",
    "\n",
    "The model was trained the dataset using Logistic Regression(tf-idf), and then ran the model against the entire death-only VAERS data, including the data from the training set (in order to capture the total of all patients who had, developed, or died of covid.)\n",
    "\n",
    "101 positive (1s) and 101 negative (0s) were marked for SYMPTOM_TEXT_TARGET\n",
    "105 positive (1s) and 104 negative (0s) were marked for LAB_DATA_TARGET\n",
    "\n",
    "From a randomly selected run from \"SYMPTOM TEXT\", 12 out of 233 were incorrect (0.0482)\n",
    "Assuming a 5% error rate when using .80 from the \"predict_prob\" column as the cutoff \n",
    "\n",
    "\n",
    "# Results:\n",
    "Out of 5184 reports of deaths after vaccination ...<br>\n",
    "\n",
    "**Average number of covid positives in VAERS data (deaths only):  240.06499999999997**\n",
    "\n",
    "**Average percent of covid positives in VAERS data (deaths only):  4.63088348765432**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
